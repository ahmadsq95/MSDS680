{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Reinforcement Learning\n",
    "\n",
    "<img align=\"right\" style=\"padding-right:10px;\" src=\"figures_wk8/reinforcement_learning.png\" width=400><br>\n",
    "\n",
    "**FTE Overview:**\n",
    "* Reinforcement Learning (RL)\n",
    "   - RL Objective\n",
    "   - RL Algorithms\n",
    "   - RL Example\n",
    "* Q-Learning\n",
    "   - What's this 'Q'?\n",
    "   - Q-Learning Algorithm\n",
    "* Demo: Q-Learning\n",
    "   - Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning (RL)\n",
    "**Reinforcement Learning (RL)** is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
    "\n",
    "RL differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Objective\n",
    "<img align=\"right\" style=\"padding-right:10px;\" src=\"figures_wk8/rl_components.png\" width=600><br>\n",
    "\n",
    "Reinforcement Learning is one of the most beautiful branches in Artificial Intelligence. The objective of RL is to maximize the reward of an agent by taking a series of actions in response to a dynamic environment.\n",
    "\n",
    "Reinforcement Learning is the science of making optimal decisions using experiences. Breaking it down, the process of Reinforcement Learning involves these simple steps: <br>\n",
    "\n",
    "1. Observation of the environment\n",
    "2. Deciding how to act using some strategy\n",
    "3. Acting accordingly\n",
    "4. Receiving a reward or penalty\n",
    "5. Learning from the experiences and refining our strategy\n",
    "6. Iterate until an optimal strategy is found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Algorithms\n",
    "<img align=\"right\" style=\"padding-right:10px;\" src=\"figures_wk8/rl_learning_system.png\" width=450><br>\n",
    "\n",
    "There are 2 main types of RL algorithms. They are **model-based** and **model-free**.\n",
    "\n",
    "A model-free algorithm is an algorithm that estimates the optimal policy without using or estimating the dynamics (transition and reward functions) of the environment. Whereas, a model-based algorithm is an algorithm that uses the transition function (and the reward function) in order to estimate the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Example\n",
    "Consider the scenario of teaching a dog new tricks. The dog doesn't understand our language, so we can't tell him what to do. Instead, we follow a different strategy. We emulate a situation (or a cue), and the dog tries to respond in many different ways. If the dog's response is the desired one, we reward them with snacks. Now guess what, the next time the dog is exposed to the same situation, the dog executes a similar action with even more enthusiasm in expectation of more food. That's like learning \"what to do\" from positive experiences. Similarly, dogs will tend to learn what not to do when face with negative experiences.\n",
    "\n",
    "That's exactly how Reinforcement Learning works in a broader sense:\n",
    "\n",
    "* Your dog is an \"agent\" that is exposed to the **environment**. The environment could in your house, with you.\n",
    "* The situations they encounter are analogous to a **state**. An example of a state could be your dog standing and you use a specific word in a certain tone in your living room\n",
    "* Our agents react by performing an **action** to transition from one \"state\" to another \"state,\" your dog goes from standing to sitting, for example.\n",
    "* After the transition, they may receive a **reward or penalty** in return. You give them a treat! Or a \"No\" as a penalty.\n",
    "* The **policy** is the strategy of choosing an action given a state in expectation of better outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "**Q-Learning** is a model-free reinforcement learning algorithm.\n",
    "\n",
    "Q-Learning is a values-based learning algorithm. Value based algorithms updates the value function based on an equation(particularly Bellman equation). Whereas the other type, policy-based estimates the value function with a greedy policy obtained from the last policy improvement.\n",
    "\n",
    "Q-learning is an off-policy learner. Means it learns the value of the optimal policy independently of the agent’s actions. On the other hand, an on-policy learner learns the value of the policy being carried out by the agent, including the exploration steps and it will find a policy that is optimal, taking into account the exploration inherent in the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's this 'Q'?\n",
    "The ‘Q’ in Q-Learning stands for **quality**. Quality here represents how useful a given action is in gaining some future reward.\n",
    "\n",
    "**Q-learning Definition** <br>\n",
    "\n",
    "* Q*(s,a) is the expected value (cumulative discounted reward) of doing a in state s and then following the optimal policy.\n",
    "* Q-Learning uses Temporal Differences(TD) to estimate the value of Q*(s,a). Temporal difference is an agent learning from an environment through episodes with no prior knowledge of the environment.\n",
    "* The agent maintains a table of Q[S, A], where S is the set of states and A is the set of actions.\n",
    "* Q[s, a] represents its current estimate of Q*(s,a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Algorithm\n",
    "The Q-Learning Algorithm uses the Bellman equation and takes two inputs: state (s) and action (a).\n",
    "\n",
    "<img align=\"center\" style=\"padding-right:10px;\" src=\"figures_wk8/q_learning_formula.png\" width=700><br>\n",
    "\n",
    "A bit complex yes! However, the demo below should clear things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Q-Learning\n",
    "For this demo we will watch the following three part video series:\n",
    "* https://www.youtube.com/watch?v=yMk_XtIEzH8&list=PLQVvvaa0QuDezJFIOU5wDdfy4e9vdnx-7 <br>\n",
    "* https://www.youtube.com/watch?v=Gq1Azv_B4-4 <br>\n",
    "* https://www.youtube.com/watch?v=CBTbifYx6a8 <br>\n",
    "\n",
    "These videos are accompanied by the following:.  \n",
    "* [Q-Learning introduction and Q Table - Reinforcement Learning w/ Python Tutorial p.1](https://pythonprogramming.net/q-learning-reinforcement-learning-python-tutorial/) <br>\n",
    "* [Q-Learning introduction and Q Table - Reinforcement Learning w/ Python Tutorial p.2](https://pythonprogramming.net/q-learning-algorithm-reinforcement-learning-python-tutorial/?completed=/q-learning-reinforcement-learning-python-tutorial/) <br>\n",
    "* [Q-Learning introduction and Q Table - Reinforcement Learning w/ Python Tutorial p.3](https://pythonprogramming.net/q-learning-analysis-reinforcement-learning-python-tutorial/?completed=/q-learning-algorithm-reinforcement-learning-python-tutorial/) <br>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Important::</b> In working through the code listed on the webpages, there are a number of errors that prevent the demo from performing correctly.  The code shown int he videos is correct!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Since your assignment for the week is to reproduce the demo, I will not be publishing the code associated with this demo. However, here is the result that you are going for!\n",
    "\n",
    "**Goal:** Use Q-Learning to move the car from the bottom of the hill to the finish flag.\n",
    "\n",
    "<img align=\"center\" style=\"padding-right:10px;\" src=\"figures_wk8/finished_car.png\" width=400><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: <br>\n",
    "https://www.kdnuggets.com/2019/10/mathworks-reinforcement-learning.html <br>\n",
    "https://en.wikipedia.org/wiki/Reinforcement_learning <br>\n",
    "https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c <br>\n",
    "https://pythonprogramming.net/q-learning-reinforcement-learning-python-tutorial/ <br>\n",
    "https://pythonprogramming.net/q-learning-algorithm-reinforcement-learning-python-tutorial/?completed=/q-learning-reinforcement-learning-python-tutorial/ <br>\n",
    "https://pythonprogramming.net/q-learning-analysis-reinforcement-learning-python-tutorial/?completed=/q-learning-algorithm-reinforcement-learning-python-tutorial/ <br>\n",
    "https://www.youtube.com/watch?v=yMk_XtIEzH8&list=PLQVvvaa0QuDezJFIOU5wDdfy4e9vdnx-7 <br>\n",
    "https://www.youtube.com/watch?v=Gq1Azv_B4-4 <br>\n",
    "https://www.youtube.com/watch?v=CBTbifYx6a8 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
